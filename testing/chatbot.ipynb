{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ICVA9rxoO6",
        "outputId": "4baccb9c-30b3-43f9-9e6d-5174f2653640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2eE_WrfC2MN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrJZPIF1ni_j",
        "outputId": "b0ebe1c9-109a-4703-9344-fb6523f7895a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Define a function to load the model and tokenizer\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    max_seq_length = 4096\n",
        "    dtype = None\n",
        "    load_in_4bit = True\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"devkartik/gl_llm\",\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load the model and tokenizer once\n",
        "model, tokenizer = load_model()\n",
        "\n",
        "# Alpaca prompt template\n",
        "alpaca_prompt = \"\"\"Bhagwad gita's knowledge is paired with an input that provides further context related to gita only. Write a response that appropriately answers and elaborate the request.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "def extract_response(full_text):\n",
        "    full_text = full_text.replace(\"<|end_of_text|>\",\"\")\n",
        "    response_start = full_text.find(\"### Response:\\n\")\n",
        "    if response_start != -1:\n",
        "        return full_text[response_start + len(\"### Response:\\n\"):].strip()\n",
        "    return full_text\n",
        "\n",
        "# Initialize session state for context tracking\n",
        "if 'context' not in st.session_state:\n",
        "    st.session_state.context = []\n",
        "\n",
        "# Function to format the chat history as a string\n",
        "def format_context(context):\n",
        "    return \"\\n\".join([f\"\\nYou: {chat['user_input']}\\nChatbot: {chat['response']}\" for chat in context])\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Bhagwad Gita Chatbot\")\n",
        "\n",
        "# Display chat history\n",
        "for chat in st.session_state.context:\n",
        "    st.markdown(f\"**You**: {chat['user_input']}\")\n",
        "    st.markdown(f\"**Chatbot**: {chat['response']}\")\n",
        "\n",
        "# User input\n",
        "user_input = st.text_input(\"You:\", key=\"user_input\")\n",
        "\n",
        "# Button to send input\n",
        "if st.button(\"Send\"):\n",
        "    if user_input:\n",
        "        try:\n",
        "            # Prepare the input for the model\n",
        "            combined_input = format_context(st.session_state.context)\n",
        "            combined_input += f\"\\nYou: {user_input}\\n\"\n",
        "            formatted_input = alpaca_prompt.format(combined_input, \"\")\n",
        "\n",
        "            inputs = tokenizer(\n",
        "                [formatted_input],\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "            responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
        "            response = extract_response(responses)\n",
        "\n",
        "            st.session_state.context.append({\"user_input\": user_input, \"response\": response})\n",
        "\n",
        "            st.experimental_rerun()\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            st.error(\"CUDA out of memory. Please try a shorter input or clear the context.\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "# Clear context button\n",
        "if st.button(\"Clear Context\"):\n",
        "    st.session_state.context = []\n",
        "    st.write(\"Context cleared.\")\n",
        "    st.experimental_rerun()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYyx-m_qZUC-",
        "outputId": "23441726-d6df-4dcc-d119-deaf8dd8d641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "up to date, audited 23 packages in 345ms\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "34.125.145.24\n",
            "your url is: https://cool-clowns-enter.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}